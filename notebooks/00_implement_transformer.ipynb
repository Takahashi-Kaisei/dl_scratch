{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe13e7b",
   "metadata": {},
   "source": [
    "### import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce9fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahashikaisei/mydir/transforemer_from_scratch/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896fb6f",
   "metadata": {},
   "source": [
    "おまじない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d46a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayしたときにshape, numel, dtypeがわかるようにする．神\n",
    "def custom_repr(self):\n",
    "    return f\"{tuple(self.shape)}[{self.numel()}]: {str(self.dtype).lstrip('torch.')} = {original_repr(self)}\"\n",
    "\n",
    "\n",
    "original_repr = torch.Tensor.__repr__\n",
    "torch.Tensor.__repr__ = custom_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51eb29",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b8a35",
   "metadata": {},
   "source": [
    "例のコレ\n",
    "\n",
    "<img src=\"./image/transformer.png\" width=\"400px\" alt=\"transformer\">\n",
    "\n",
    "ここではテキスト生成を目的とするので，decoder部分のみのtransformerを実装する．\n",
    "\n",
    "decoder onlyのtransformerのアーキテクチャはコレ．なんかググったら出てきた．\n",
    "\n",
    "<img src=\"./image/transformer_decoder.png\" width=\"200px\" alt=\"transformer_decoder\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b6a4b",
   "metadata": {},
   "source": [
    "## trasformer layerに入れる前の準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add9c7e",
   "metadata": {},
   "source": [
    "アーキテクチャをみると，テキストを入力してからembedding, positional encodingをしてからtransformer layerに入れている．\n",
    "\n",
    "その工程を一個一個実装していく．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb99d25",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1942e3",
   "metadata": {},
   "source": [
    "まずはテキストをtoken化する．数字じゃないと使えないので．\n",
    "\n",
    "ということでトークナイザーを準備する，\n",
    "\n",
    "また，id化したものをtensorに変換しておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb09314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)[8]: int64 = tensor([  163,   100,   223, 31676, 20185, 30640, 33623, 16764])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ここではgpt2tokenizerを使う．これが手っ取り早い．\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "sample_text = \"私はAIです。\"\n",
    "input_ids = torch.tensor(gpt2_tokenizer.encode(sample_text))\n",
    "display(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e03a0",
   "metadata": {},
   "source": [
    "id化できた．length=8だ．\n",
    "\n",
    "idは元の字に対応しているのでその対応関係を見てみる．\n",
    "\n",
    "decodeすればいいだけ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb993999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'は'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'で'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'す'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'。'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gpt2_tokenizer.decode([163, 100, 223]))\n",
    "display(gpt2_tokenizer.decode([31676]))\n",
    "display(gpt2_tokenizer.decode([20185]))\n",
    "display(gpt2_tokenizer.decode([30640]))\n",
    "display(gpt2_tokenizer.decode([33623]))\n",
    "display(gpt2_tokenizer.decode([16764]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d909e",
   "metadata": {},
   "source": [
    "謎に\"私\"で3つidが割り当てられている．これはなんでだろう？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799a949",
   "metadata": {},
   "source": [
    "それは，各idは以下のように対応しているのが答え．\n",
    "\n",
    "私にidが3つ振られているのは，unicodeの組み合わせで表現されているから．\n",
    "\n",
    "<img src=\"./image/tokenize_example.png\" width=\"400px\" alt=\"tokenize_example\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d7f4e",
   "metadata": {},
   "source": [
    "unicodeと文字が一対一対応ならレンダリングされるけど，組み合わせでできてる\"私\"は潰れてしまってる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231f89a",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc22f14",
   "metadata": {},
   "source": [
    "tokenizeできたら，次はembeddingをする．\n",
    "\n",
    "tokenを一個一個ベクトルにして，計算処理ができるようにする．\n",
    "\n",
    "ちなみにここでできるベクトルは最初は特に意味がないランダムなベクトル．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502ba9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = (\n",
    "    gpt2_tokenizer.vocab_size\n",
    ")  # tokenizerの語彙数．出力時には，これを確率分布で表現する．\n",
    "display(vocab_size)  #\n",
    "d_model = 512  # 埋め込み次元数，attention is all you needに合わせる．\n",
    "embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abbd8d",
   "metadata": {},
   "source": [
    "この埋め込みベクトルは学習によって言語の分散表現を獲得する．\n",
    "\n",
    "したがってparameters()で取得できる．\n",
    "\n",
    "だから学習可能パラメータを定義していると考えればいいね．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd2dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352e0cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " (50257, 512)[25731584]: float32 = tensor([[ 1.5396,  0.0155,  1.2735,  ..., -0.7754, -0.0243,  0.1280],\n",
       "         [-0.1923,  0.0127, -0.4817,  ...,  0.3117,  1.7169,  0.8467],\n",
       "         [-0.1210, -1.0940, -1.6180,  ...,  0.0884,  0.4523,  0.6500],\n",
       "         ...,\n",
       "         [ 0.0510, -0.9768,  0.6745,  ...,  0.2911,  1.4849,  0.9392],\n",
       "         [ 1.6317,  1.3134, -0.2387,  ..., -0.3693, -0.5047, -0.0954],\n",
       "         [ 0.8078,  1.4899,  0.4799,  ...,  0.5571, -1.6068,  0.1679]],\n",
       "        requires_grad=True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(list(embed.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9527c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_input = embed(input_ids)\n",
    "embed_input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a791a4a",
   "metadata": {},
   "source": [
    "もちろん，8個のidを振られた文章をembeddingしているので，idを取得すると(8, 512)となっている．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed42091",
   "metadata": {},
   "source": [
    "### positional encoding\n",
    "\n",
    "Attention is all you need 論文では，位置情報を加えるためにsine/cosine関数を用いた位置エンコーディングを使っている．\n",
    "\n",
    "しかし，ここでは簡単のために，単純に学習可能なパラメータとして位置エンコーディングを実装する．\n",
    "\n",
    "positional encodingは，単純にembeddingされたベクトルに足し合わせるだけ．\n",
    "\n",
    "位置ごとに割り振られた学習可能パラメータをただ足すだけで良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95848f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_input.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697221b",
   "metadata": {},
   "source": [
    "上記のembed_inputがpostional encodingをする前のベクトル．これに位置情報を足す．\n",
    "\n",
    "なのでsizeは変わらない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ef01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = 100  # コンテキストウィンドウ．tokenいくつまでを処理するか．プロプライエタリモデルのドキュメントを見ると，絶対かいてある．これ以上はカットされる．\n",
    "# ここでは100tokenしか処理しない．本当はもっと大きいよ．\n",
    "d_model = embed_input.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54ca006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(100, 512)[51200]: float32 = tensor([[-1.4986, -2.6413,  0.4043,  ..., -0.3874,  0.9318,  0.1330],\n",
       "        [ 0.7439,  0.3280,  1.0592,  ..., -0.3740,  0.6549,  0.8521],\n",
       "        [ 0.5973, -0.2180,  0.2851,  ..., -1.6767, -0.5493,  0.3618],\n",
       "        ...,\n",
       "        [ 1.3827,  0.9211,  1.6553,  ..., -0.6416, -1.6116, -1.2377],\n",
       "        [ 0.6457, -1.5711, -0.6449,  ..., -0.5333,  2.4517, -2.6644],\n",
       "        [ 1.1070, -0.7889,  0.2765,  ...,  1.7800,  0.9920,  0.7272]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pe = nn.Parameter(torch.randn([context_window, d_model]))\n",
    "display(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9a884",
   "metadata": {},
   "source": [
    "(100, 50257)のベクトルができた．\n",
    "\n",
    "単純にスライスして，必要な長さだけをembed_inputに足し合わせれば良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f2f003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 512)[4096]: float32 = tensor([[ 1.4865, -2.0641, -2.9895,  ..., -0.4920,  2.0101, -2.2965],\n",
       "        [ 0.7928, -1.0908, -0.6428,  ...,  1.1935,  0.4169, -1.3922],\n",
       "        [-0.1188, -3.1827, -0.9049,  ...,  0.7935, -0.6548, -2.3558],\n",
       "        ...,\n",
       "        [-1.4752, -0.3560,  0.2262,  ...,  1.8922,  0.6387, -1.4002],\n",
       "        [-1.1464, -0.4868, -1.3613,  ...,  0.6582,  2.6510, -1.4161],\n",
       "        [ 1.8512,  0.1382, -1.3188,  ...,  1.2771, -0.5142, -1.2817]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_input + pe[embed_input.size()[0], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e5ddf",
   "metadata": {},
   "source": [
    "これでtransformer layerに入れる準備ができた．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f40faa",
   "metadata": {},
   "source": [
    "## transformer layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0d550",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76484d",
   "metadata": {},
   "source": [
    "いよいよtransformer layerを実装する．\n",
    "\n",
    "まずscaled dot-product attentionを実装することから始める．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71dcfa",
   "metadata": {},
   "source": [
    "やることはいたってシンプル．\n",
    "\n",
    "<img src=\"./image/scaled_dot_product_attention.png\" width=\"400px\" alt=\"scaled_dot_product_attention.png\">\n",
    "\n",
    "もとい，\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "をそのまま書くだけ．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb380091",
   "metadata": {},
   "source": [
    "数式を実装するにあたって，どうすればいいかというと，右辺の式でしか登場しない変数はコンストラクタで定義して，左辺で渡されている変数はforwardの引数にすればいい．\n",
    "\n",
    "これがコツ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.scale = d ** (\n",
    "            1 / 2\n",
    "        )  # 右辺で新たに渡している要素はdのみ．コンストラクタで定義する．\n",
    "        self.softmax = nn.Softmax(\n",
    "            dim=-1\n",
    "        )  # batchが入ってきたときに対応できるよう，-1にしておく．softmaxはkey方向に取りたい．それは横方向．\n",
    "        # ここが，dim=1だとbatch次元が増えたときに，一個ずれてしまう．不本意に縦方向をとってしまう．\n",
    "\n",
    "    # 左辺をそのまま書く．左辺で新たに渡している要素はQ, K, Vだよね．\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q: seq_length × embedding_size(8×512)\n",
    "            k: seq_length × embedding_size(8×512)\n",
    "            v: seq_length × embedding_size(8×512)\n",
    "        \"\"\"\n",
    "        score = (q @ k.T) / self.scale  # 8×8\n",
    "        return self.softmax(score) @ v  # 8×8 @ 8×512 = 8×512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44ea24",
   "metadata": {},
   "source": [
    "双方向のattentionが実装できた．\n",
    "\n",
    "しかし，双方向のattentionはgpt系のモデルでは使わない．いわゆるcausal attentionを実装する必要がある．\n",
    "\n",
    "なにをするかというと，未来の情報を参照しないようにmaskをかける．\n",
    "\n",
    "その前にbatch化したときに問題なく実装できるように，tensorの挙動を確認しておこう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e9905ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 512])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.rand(2, 8, 512)  # (batch, seq_len, d_model)を想定\n",
    "display(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af12c97",
   "metadata": {},
   "source": [
    "これを転置したい．\n",
    "\n",
    "今までは`.T`で良かったけど，batch軸が足されたtensorの場合どうだろうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00295b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 8, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x.T.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffa939",
   "metadata": {},
   "source": [
    "batch軸, d_model軸で入れ替えられた．\n",
    "\n",
    "これは不本意．本当はseq_lenとd_modelの軸を入れ替えたい．\n",
    "\n",
    "それは`.mT`をやるとうまくいく．mTはおしりの二軸を入れ替える操作．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b642c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x.mT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f57c93",
   "metadata": {},
   "source": [
    "現状の双方向のattentionだと，queryが渡された時に未来の情報がリークしている．\n",
    "\n",
    "このままだと，未来の情報を参照してしまうので，次単語予測がうまく学習できない．\n",
    "\n",
    "どういうことか．\n",
    "\n",
    "\"私はAIです。\"という文章があり，この文章がそのままquery, keyに対応するとする．\n",
    "\n",
    "この時,query, keyのマトリクスは以下．\n",
    "| query\\key  | 私 | は | A | I | です | 。 |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 私 ||||||\n",
    "| は ||||||\n",
    "| A ||||||\n",
    "| I ||||||\n",
    "| です ||||||\n",
    "| 。 ||||||\n",
    "\n",
    "双方向の例は以下．\n",
    "\n",
    "| query\\key  | 私 | は | A | I | です | 。 |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 私 ||||||\n",
    "\n",
    "このようにqueryに\"私\"が来ているのに，keyでそれ以降の未来の単語情報を与えてしまうのはリークしている．\n",
    "\n",
    "この状態だとまともに学習もできない．\n",
    "\n",
    "単方向にする必要がある．\n",
    "\n",
    "単方向にするにはmaskをかければ良い．\n",
    "\n",
    "| query\\key  | 私 | は | A | I | です | 。 |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 私 ||///|///|///|///|///\n",
    "| は |||///|///|///|///\n",
    "| A ||||///|///|///\n",
    "| I |||||///|///\n",
    "| です ||||||///\n",
    "| 。 ||||||\n",
    "\n",
    "この///の部分はattentionをとらない．\n",
    "\n",
    "そうすれば未来の情報を参照しなくなる．\n",
    "\n",
    "これがmaskのイメージ．\n",
    "\n",
    "実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "820256f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask=(8, 8)[64]: bool = tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# 全ての要素が1の行列を作って，上三角行列を飛ばしてboolにする．Trueの場所を-infにするのがmask\n",
    "seq_len = 8\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "display(mask.shape)\n",
    "print(f\"{mask=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fed235",
   "metadata": {},
   "source": [
    "で，バッチ軸を足したときに，このtensorはうまくブロードキャストしてくれるのかが気になる．\n",
    "\n",
    "→してくれる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ee6870f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# こういうのが欲しい．\n",
    "seq_len = 8\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "display(mask.shape)\n",
    "unsqueezed = mask.unsqueeze(0)  # batch軸を足す操作．indexを指定．\n",
    "display(unsqueezed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e4491d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 8)[64]: bool = tensor([[[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b213d7",
   "metadata": {},
   "source": [
    "causal attentionを作る．\n",
    "\n",
    "単方向のattentionはcausal attentionと呼ばれる．\n",
    "\n",
    "まあ式は一緒．\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccdee2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.scale = d ** (1 / 2)  # 右辺で新たに渡している要素はdのみ\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q: (seq_length, embedding_size)\n",
    "            k: (seq_length, embedding_size)\n",
    "            v: (seq_length, embedding_size)\n",
    "        \"\"\"\n",
    "        seq_len = q.shape[-2]\n",
    "        score = (q @ k.mT) / self.scale\n",
    "        # 増えたところ↓\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        score = score.masked_fill(mask, -torch.inf)  # Trueを-infに飛ばす．\n",
    "\n",
    "        return self.softmax(score) @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2c993",
   "metadata": {},
   "source": [
    "attentionのイメージを完璧に掴むため，自力で計算してみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18ba7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "q = torch.tensor([[1, 0], [1, 1], [2, 2]]).unsqueeze(0).to(torch.float32)\n",
    "k = torch.tensor([[1, 0], [1, 1], [2, 2]]).unsqueeze(0).to(torch.float32)\n",
    "v = torch.tensor([[5, 0], [5, 5], [-3, 100]]).unsqueeze(0).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c444a",
   "metadata": {},
   "source": [
    "以下のような値がq, k, vであったとき，どういうoutputになるか．\n",
    "\n",
    "計算の簡単のために，`scale = 1`で, softmaxではなく，重みが均等な確率分布の処理を考える．\n",
    "\n",
    "一単語ごとに内積を計算する．\n",
    "\n",
    "- query一個目[1, 0]の処理\n",
    "\n",
    "`q = [1, 0]`がのとき，対応するkは`k = [1, 0]`\n",
    "\n",
    "$$\n",
    "q\\cdot k^T = 1 \\\\\n",
    "\n",
    "1\\cdot v = 1\\cdot [5, 0] = [5, 0]\n",
    "$$\n",
    "\n",
    "- query二個目[1, 1]の処理\n",
    "\n",
    "`q = [1, 1]`がのとき，対応するkは`k = [1, 0], [1, 1]`\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q\\cdot k^T &= \\begin{bmatrix} 1 , 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 , 1 \\\\ 0, 1 \\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} 1 , 2 \\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "確率分布にする．\n",
    "$$\n",
    "\\begin{bmatrix} 0.33, 0.66 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 0.33, 0.66 \\end{bmatrix} \\times v = 0.33\\cdot [5, 0] + 0.66\\cdot [5, 5] = [5.0, 3.3]\n",
    "$$\n",
    "\n",
    "- query三個目[2, 2]の処理\n",
    "\n",
    "`q = [2, 2]`がのとき，対応するkは`k = [1, 0], [1, 1], [2, 2]`\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q\\cdot k^T &= \\begin{bmatrix} 2 , 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 , 0 \\\\ 1, 1 \\\\ 2, 2 \\end{bmatrix}\\\\\n",
    "q\\cdot k^T &= \\begin{bmatrix} 2 , 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1, 1, 2 \\\\ 0, 1, 2 \\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} 2 , 4, 8 \\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "確率分布にする．\n",
    "$$\n",
    "    \\begin{bmatrix} 0.14, 0.28, 0.57 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} 0.14, 0.28, 0.57 \\end{bmatrix} \\times v = 0.14\\cdot [5, 0] + 0.28\\cdot [5, 5] +  0.57\\cdot [-3, 100]= [0.39, 58.4]\n",
    "$$\n",
    "\n",
    "まとめると...\n",
    "\n",
    "$$\n",
    "\\text{CausalAttention}(Q, K, V) = \\begin{bmatrix} 5.0, 0.0 \\\\ 5.0, 3.3 \\\\ 0.39, 58.4 \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f08d5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)[6]: float32 = tensor([[[ 5.0000,  0.0000],\n",
       "         [ 5.0000,  3.6553],\n",
       "         [-2.8370, 98.0526]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = CausalAttention(1)\n",
    "y = attention(q, k, v)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ad69f",
   "metadata": {},
   "source": [
    "実際の答えとはズレた，コレはsoftmaxが原因．\n",
    "\n",
    "query3個目の確率分布は[0.14, 0.28, 0.57]ではなく，softmaxで計算すると[0.00, 0.01, 0.97]になる．\n",
    "\n",
    "ここで結構ズレるので最終的な結果も大幅にズレた．\n",
    "\n",
    "でもAttentionの気持ちはコレで掴めた．\n",
    "\n",
    "query一個に着目して，それとkeyの内積をとる．内積の値がそのままscore(単語同士の重要度みたいなもん)で，それを確率分布にする．\n",
    "\n",
    "確率分布でvalueのベクトルごとの重要度を操作．\n",
    "\n",
    "最後に和をとる．(いわゆる重み付き和を計算することになっている．)\n",
    "\n",
    "ちなみにtorchで実装されてるやつと実装はちゃんと合ってる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4b73c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)[6]: float32 = tensor([[[ 5.0000,  0.0000],\n",
       "         [ 5.0000,  3.6553],\n",
       "         [-2.8370, 98.0526]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "F.scaled_dot_product_attention(q, k, v, is_causal=True, scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a81a82",
   "metadata": {},
   "source": [
    "ポイントは時刻tごとの演算をすることなのです．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c21637",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1b2d4",
   "metadata": {},
   "source": [
    "コレ．\n",
    "\n",
    "<img src=\"./image/mha.png\" width=\"300px\" alt=\"mha.png\">\n",
    "\n",
    "数式は以下．\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_{\\text{h}})W^O\\\\\n",
    "\\text{where}\\text{ } \\text{head}_{\\text{i}} = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6a31c",
   "metadata": {},
   "source": [
    "これも簡単で，埋め込みベクトルをhead数で分割して，それごとにAttentionを計算．\n",
    "\n",
    "headに分割したのを元に戻して線形層を噛ませる．\n",
    "\n",
    "てかそもそもQ, K, Vってどうやって作るねんって話だよね．\n",
    "\n",
    "以下の図をみればわかる．\n",
    "\n",
    "<img src=\"./image/transformer_decoder.png\" width=\"200px\" alt=\"transformer_decoder\">\n",
    "\n",
    "一個の入力から分岐してる．\n",
    "\n",
    "つまり入力の(batch_size, seq_len, d_model)のベクトルをそのまま使ってんだね．\n",
    "\n",
    "これを線形層に通して，Attentionに突っ込む．\n",
    "\n",
    "$QW^Q_i, KW^K_i, VW^V_i$って書いてあって，パラメータの行列がかかってるけど，意味は線形層と全く一緒．\n",
    "\n",
    "なので`torch.nn.Linear`で実装すれば良い．\n",
    "\n",
    "アーキテクチャ的には，headに分割してから線形層に通してるけど，線形層に通してからheadに分割しても同じなので，後者の方が実装は楽．それで実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9667704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_head, d_model\n",
    "    ):  # ここは64じゃなくてn_headの方が良い．契約プログラミング的に．64とすると全体で見た時マジックナンバーっぽくなる．\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, \"割り切れないよ\"\n",
    "        self.n_head = n_head\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.attention = CausalAttention(d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        qw = self.w_q(q)\n",
    "        kw = self.w_k(k)\n",
    "        vw = self.w_v(v)\n",
    "        calculated = []\n",
    "        # chunkでhead数に分割する．\n",
    "        for qw_i, kw_i, vw_i in zip(\n",
    "            qw.chunk(self.n_head, dim=-1),\n",
    "            kw.chunk(self.n_head, dim=-1),\n",
    "            vw.chunk(self.n_head, dim=-1),\n",
    "        ):\n",
    "            calculated.append(self.attention(qw_i, kw_i, vw_i))\n",
    "        return self.w_o(torch.cat(calculated, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "795c5176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_param=263168\n"
     ]
    }
   ],
   "source": [
    "n_head = 8\n",
    "multi_head_attention = MultiHeadAttention(n_head, 256)\n",
    "n_param = sum(\n",
    "    p.numel() for p in multi_head_attention.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"{n_param=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0275e1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_param=263168\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10\n",
    "d_model = 256\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "multi_head_attention(q, k, v)\n",
    "n_param = sum(\n",
    "    p.numel() for p in multi_head_attention.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"{n_param=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55158267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数: 263168\n"
     ]
    }
   ],
   "source": [
    "mha = nn.MultiheadAttention(d_model, n_head)\n",
    "mha(q, k, v)\n",
    "\n",
    "n_param = sum(p.numel() for p in mha.parameters() if p.requires_grad)\n",
    "print(f\"パラメータ数: {n_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cc0a2",
   "metadata": {},
   "source": [
    "できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923309b",
   "metadata": {},
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833782ee",
   "metadata": {},
   "source": [
    "multi head attentionの後はFeed Forward Network(FFN)を通す．\n",
    "\n",
    "ここに記憶が詰まるとか言われてんだっけね．\n",
    "\n",
    "数式は以下．\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "この数式の意味は線形層に通して，その後にReLUを通して，さらに線形層に通してるだけ．\n",
    "\n",
    "隠れ層の次元数はd_ffとして記述されていて，論文中では2048に設定されている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3a085a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30282f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 2)[42]: float32 = tensor([[[ 0.2097, -0.0288],\n",
       "         [ 0.1410, -0.0016],\n",
       "         [ 0.1480, -0.1233],\n",
       "         [ 0.1540,  0.0016],\n",
       "         [ 0.2656, -0.0517],\n",
       "         [ 0.1744, -0.0264],\n",
       "         [ 0.2467, -0.0439]],\n",
       "\n",
       "        [[ 0.1932, -0.0204],\n",
       "         [ 0.1925, -0.0477],\n",
       "         [ 0.1425, -0.0987],\n",
       "         [ 0.1447, -0.0189],\n",
       "         [ 0.1603, -0.0509],\n",
       "         [ 0.2292, -0.0801],\n",
       "         [ 0.2369, -0.0392]],\n",
       "\n",
       "        [[ 0.2678, -0.0530],\n",
       "         [ 0.1079, -0.1322],\n",
       "         [ 0.2106, -0.0277],\n",
       "         [ 0.1432,  0.0058],\n",
       "         [ 0.1351, -0.0789],\n",
       "         [ 0.2769, -0.0792],\n",
       "         [ 0.1987, -0.1195]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FFN(2, 8)\n",
    "ffn(torch.rand(3, 7, 2))  # batch_size, seq_len, d_model\n",
    "# batch_firstのオプションもある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3be052",
   "metadata": {},
   "source": [
    "### 合わせる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515b604",
   "metadata": {},
   "source": [
    "これでtransformer layerの全ての構成要素が揃った．\n",
    "\n",
    "これらを組み合わせる．\n",
    "\n",
    "素直にそのまま書くだけ．\n",
    "\n",
    "インスタンス変数には，今まで出てきたインスタンス変数を持たせる．\n",
    "\n",
    "また，residual connectionとpost layer normalizationも忘れずに入れる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f8fb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_head, d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.layer_norm_1 = nn.LayerNorm(\n",
    "            d_model\n",
    "        )  # normレイヤーはパラメータを共有しているわけでは無いので，二つインスタンスが必要．\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):  # (batch, seq_len, d_model)\n",
    "        residual_1 = x  # 接続用に保存しておく．\n",
    "        x = self.mha(x)\n",
    "        x = x + residual_1  # 残差結合\n",
    "        x = self.layer_norm_1(x)  # post norm\n",
    "        residual_2 = x\n",
    "        x = self.ffn(x)\n",
    "        x = x + residual_2\n",
    "        x = self.layer_norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "450260b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e35e1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLayer(8, 512, 2048)\n",
    "y = model(torch.rand(4, 8, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "846e9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TransformerLayer.png'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = make_dot(y, params=dict(model.named_parameters()))\n",
    "image.format = \"png\"\n",
    "image.render(\"TransformerLayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb242f",
   "metadata": {},
   "source": [
    "## TransformerClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189a9ab",
   "metadata": {},
   "source": [
    "transformerlayerを積み重ねて，TransformerClassを作る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1169072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, n_head, d_model, d_ff, n_layers, context_window\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=d_model\n",
    "        )\n",
    "        self.transformer_layers = nn.Sequential(\n",
    "            *[TransformerLayer(n_head, d_model, d_ff) for _ in range(n_layers)]\n",
    "        )  # いけてる？\n",
    "        self.linear = nn.Linear(\n",
    "            d_model, vocab_size\n",
    "        )  # batch_size, seq_len, vocab_sizeになる\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.pe = nn.Parameter(\n",
    "            torch.randn(\n",
    "                [context_window, d_model]\n",
    "            )  # ここvocab_sizeじゃなくてd_modelじゃね？\n",
    "        )  # 上限までベクトルを作って，forwardで削れば良い．\n",
    "\n",
    "    def forward(self, x):  # batch_size, seq_len: int\n",
    "        x = self.embed(x)  # batch_size, seq_len, d_model\n",
    "        x = x + self.pe[:, x.size(-2), :]\n",
    "        x = self.transformer_layers(x)\n",
    "        x = self.linear(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "417d9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = gpt2_tokenizer.vocab_size\n",
    "n_head = 8\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "context_window = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_param=70,952,017\n",
      "model_size=283,808,068\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    vocab_size, n_head, d_model, d_ff, n_layers, context_window\n",
    ")\n",
    "\n",
    "# transformer\n",
    "n_param = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "print(f\"{n_param=:,}\")\n",
    "# 1億2000万のパラメータ数！！！\n",
    "# float32の場合，1パラメータ4バイト換算される．\n",
    "model_size = n_param * 4\n",
    "print(f\"{model_size=:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "901cdbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_param=18,914,304\n"
     ]
    }
   ],
   "source": [
    "true_transformer = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(\n",
    "        d_model, n_head, d_ff, batch_first=True, dropout=0.0\n",
    "    ),\n",
    "    num_layers=n_layers,\n",
    ")\n",
    "n_param = sum(\n",
    "    p.numel() for p in true_transformer.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"{n_param=:,}\")\n",
    "# embedding, pe, fcを除いた場合と同じパラメータ数！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a01de",
   "metadata": {},
   "source": [
    "input: `(batch_size, seq_len)` ※ int (idになってる)\n",
    "\n",
    "transformerlayer: `(batch_size, seq_len, d_model)`\n",
    "\n",
    "output: `(batch_size, seq_len, vocab_size)` ※ 次vocab次元のベクトルになっていればOK\n",
    "\n",
    "\n",
    "例としては，\n",
    "\n",
    "”今日もいい天気ですね”がtoken化され，”今日\", \"も\", ”いい”, \"天気\", \"です\"，”ね”になり，これらが入力されるとしたら，transformerの出力は，\n",
    "\n",
    "- ”今日”の次の単語の確率分布\n",
    "-   ”今日も”の次の単語の確率分布\n",
    "-   ”今日もいい”の次の単語の確率分布\n",
    "-   ”今日もいい天気”の次の単語の確率分布\n",
    "-   ”今日もいい天気です”の次の単語の確率分布\n",
    "-   ”今日もいい天気ですね”の次の単語の確率分布\n",
    "\n",
    "ということになる．\n",
    "\n",
    "この確率分布は，vocab_size次元のベクトルで表現される．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db131698",
   "metadata": {},
   "source": [
    "確認する．\n",
    "\n",
    "transformerを一つの線形層で表現するなら以下のようになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "420ec1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 7)[84]: float32 = tensor([[[ 0.1921,  0.0530,  0.0252,  0.4259,  0.4248, -0.3778,  0.5878],\n",
       "         [ 0.0192,  0.0229,  0.1247,  0.2860,  0.1657, -0.2833,  0.6537],\n",
       "         [ 0.1208,  0.0741,  0.2268, -0.0341,  0.2610, -0.0129,  0.3781]],\n",
       "\n",
       "        [[ 0.1997,  0.0543,  0.2180,  0.0420,  0.4981,  0.0539,  0.3069],\n",
       "         [ 0.1565, -0.0918,  0.3026,  0.3859,  0.4636, -0.3343,  0.4118],\n",
       "         [-0.0115, -0.0487,  0.2160,  0.2810,  0.1767, -0.1957,  0.6262]],\n",
       "\n",
       "        [[ 0.1670, -0.2004,  0.5206,  0.2835,  0.6557, -0.0684,  0.1956],\n",
       "         [ 0.1638,  0.1049,  0.0354,  0.2540,  0.3967, -0.2199,  0.5378],\n",
       "         [ 0.1319, -0.2266,  0.4141,  0.2749,  0.2086, -0.1107,  0.4553]],\n",
       "\n",
       "        [[ 0.2336, -0.0559,  0.3827,  0.2375,  0.6575, -0.2093,  0.1862],\n",
       "         [ 0.1759, -0.1723,  0.4199,  0.4498,  0.5762, -0.3621,  0.3189],\n",
       "         [ 0.2337, -0.0123,  0.3160,  0.0439,  0.4091, -0.0409,  0.2558]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, d_model = 4, 3, 5\n",
    "vocab_size = 7\n",
    "sample = torch.rand([batch_size, seq_len, d_model])\n",
    "linear = nn.Linear(d_model, vocab_size)  # shapeの最後の次元が変換される．-1軸\n",
    "output = linear(sample)\n",
    "output.reshape(batch_size, seq_len, vocab_size)\n",
    "\n",
    "# seq_len, d_model => seq_len, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328312f",
   "metadata": {},
   "source": [
    "数式的に...\n",
    "$$\n",
    "input: x_1, x_2, ... x_n\\\\\n",
    "$$\n",
    "$$\n",
    "model: p(x_t|x_{(t-1)}, x_{(t-2)},...,x_{(1)}) \\text{ }※ p(x_t|x_{<t}) これがほしい\n",
    "$$\n",
    "\n",
    "inputに対するモデルの対数尤度\n",
    "p(x_t|x_<t)に全てのxを入れる場合の対数尤度\n",
    "\n",
    "$$\n",
    "L = \\log\\prod_{t=1}^n p(x_t|x_{<t})\n",
    "\n",
    "= \\sum_{t=1}^n\\log p(x_t|x_{<t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac489d66",
   "metadata": {},
   "source": [
    "input x_1, x_2, x_3, x_4: int\n",
    "\n",
    "output (確率分布が出力される．これがマスクをかけた意味！)\n",
    "- p(x_1|BOS)\n",
    "- p(x_2|x_1)\n",
    "- p(x_3|x_1, x_2)\n",
    "- p(x_4|x_1, x_2, x_3)\n",
    "\n",
    "output shape\n",
    "(batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c50ac8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fcbdf",
   "metadata": {},
   "source": [
    "# 雑\n",
    "\n",
    "参考： https://github.com/misya11p/language-models/tree/main?tab=readme-ov-file\n",
    "\n",
    "- [小話: 機械学習系の論文実装で気をつけて読むべきほぼ唯一の箇所, 行列演算](https://note.com/cute_orchid39/n/n68e181a041fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"今日\": {\"は\": 1},\n",
    "    \"カレー\": {\"は\": 1},\n",
    "    \"天気\": {\"は\": 0.5, \"。\": 0.5},\n",
    "    \"おいしい\": {\"。\": 0.5, \"カレー\": 0.5},\n",
    "    \"は\": {\"今日\": 0.25, \"カレー\": 0.25, \"おいしい\": 0.25, \"いい\": 0.25},\n",
    "    \"いい\": {\"天気\": 0.5, \"。\": 0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372adabe",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=mMa2PmYJlCo\n",
    "\n",
    "https://bbycroft.net/llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a3418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'今日': {'は': 0.6666666666666666, 'カレー': 0.3333333333333333},\n",
       " 'は': {'いい': 0.25, 'カレー': 0.5, '今日': 0.25},\n",
       " 'いい': {'天気': 1.0},\n",
       " '天気': {'です': 1.0},\n",
       " 'です': {'。': 1.0},\n",
       " 'カレー': {'を': 0.6666666666666666, 'が': 0.3333333333333333},\n",
       " 'を': {'食べ': 1.0},\n",
       " '食べ': {'ました': 1.0},\n",
       " 'ました': {'。': 1.0},\n",
       " '私': {'は': 1.0},\n",
       " 'が': {'好き': 1.0},\n",
       " '好き': {'です': 1.0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"今日 は いい 天気 です 。\",\n",
    "    \"今日 は カレー を 食べ ました 。\",\n",
    "    \"私 は 今日 カレー を 食べ ました 。\",\n",
    "    \"私 は カレー が 好き です 。\",\n",
    "]\n",
    "\n",
    "vocab = {}\n",
    "for sent in data:  # 文章ごとに分ける\n",
    "    sent = sent.split()  # 単語ごとに分ける\n",
    "    for w1, w2 in zip(\n",
    "        sent[:-1], sent[1:]\n",
    "    ):  # w1は最後の単語を除いたもの，w2は最初の単語を除いたもの\n",
    "        # 一個ずつずらしてペアにする．\n",
    "        if w1 not in vocab:\n",
    "            vocab[w1] = {}\n",
    "        if w2 not in vocab[w1]:\n",
    "            vocab[w1][w2] = 0\n",
    "        vocab[w1][w2] += 1\n",
    "    # このfor文では，w1を探して，vocabに入ってなかったら，空の辞書を入れる．\n",
    "    # さらに，w2を探して，vocab[w1]にw2がなかったら，0を入れる．\n",
    "    # 最後に，w1かつつw2のkeyの組み合わせが出てきたら，1を足す．\n",
    "    # なのでここでは頻度を出してるだけ．\n",
    "\n",
    "# 確率分布にスケールを揃える．\n",
    "# 辞書にアクセスしてvalues()を持ってくれば，値のみ取り出せる．それをsumればいいだけ．\n",
    "for w1 in vocab:\n",
    "    total = sum(vocab[w1].values())\n",
    "    for w2 in vocab[w1]:\n",
    "        vocab[w1][w2] /= total\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba96c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe6286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日 は いい 天気 です 。',\n",
       " '今日 は カレー を 食べ ました 。',\n",
       " '私 は 今日 カレー を 食べ ました 。',\n",
       " '私 は カレー が 好き です 。']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 は 今日 は いい 天気 です 。\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"今日 は いい 天気 です 。\",\n",
    "    \"今日 は カレー を 食べ ました 。\",\n",
    "    \"私 は 今日 カレー を 食べ ました 。\",\n",
    "    \"私 は カレー が 好き です 。\",\n",
    "]\n",
    "display(data)\n",
    "model = markovify.Text(data, state_size=1)  # 学習\n",
    "sentence = model.make_sentence()\n",
    "print(sentence)\n",
    "\n",
    "# Noneで終わるケースはどういうケース？句読点に行きつかなかったってこと？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c7c3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):  # nn.Moduleを継承\n",
    "    # 語彙数, 埋め込みサイズ\n",
    "    def __init__(self, n_vocab: int, hidden_size: int):\n",
    "        super().__init__()  # 親クラスの初期化\n",
    "        self.embedding = nn.Embedding(n_vocab, hidden_size)  # 埋め込み\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size, n_vocab\n",
    "        )  # 埋め込みから語彙数への線形変換\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.embedding(x)  # (batch_size, hidden_size)\n",
    "        y = self.fc(h)  # (batch_size, n_vocab)\n",
    "        return y\n",
    "\n",
    "\n",
    "# これの出力ってなんなの？埋め込んだものをもう一回線形変換してるだけじゃん．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e383c8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m n_vocab = \u001b[38;5;28mlen\u001b[39m(\u001b[43msp\u001b[49m)\n\u001b[32m      2\u001b[39m embed_dim = \u001b[32m512\u001b[39m\n\u001b[32m      3\u001b[39m model = LanguageModel(n_vocab, embed_dim)\n",
      "\u001b[31mNameError\u001b[39m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "embed_dim = 512\n",
    "model = LanguageModel(n_vocab, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64000000 ** (1 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac314f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8007.8125"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8200000 / 512 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言語モデルの学習\n",
    "loss_fn = nn.CrossEntropyLoss()  # 損失関数\n",
    "\n",
    "\n",
    "# モデルを評価する時．\n",
    "def eval_model(model):\n",
    "    model.eval()  # 評価モード\n",
    "    losses = []  # 損失をためておく\n",
    "    with torch.no_grad():  # 勾配を計算しない．\n",
    "        for x, t in test_loader:  # テストデータを，ミニバッチごとに取り出す．\n",
    "            x = x.to(device)  # 訓練データ\n",
    "            t = t.to(device)  # 正解\n",
    "            y = model(x)  # 出力\n",
    "            loss = loss_fn(y, t)  # 損失をとる．\n",
    "            losses.append(\n",
    "                loss.item()\n",
    "            )  # item()でスカラーに変換してからappendする．\n",
    "    loss = sum(losses) / len(losses)  # ミニバッチごとの損失の平均をとる．\n",
    "    ppl = torch.exp(torch.tensor(loss)).item()  # perplexityに変換．\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(\n",
    "        n_iter=len(train_loader),\n",
    "        n_epochs=n_epochs,\n",
    "        unit=prog_unit,\n",
    "        label=\"ppl train\",\n",
    "        agg_fn=lambda s, w: math.exp(s / w),  # ppl\n",
    "    )\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            optimizer.zero_grad()  # 勾配初期化\n",
    "            x = x.to(device)  # 入力\n",
    "            t = t.to(device)  # 正解\n",
    "            y = model(x)  # 出力\n",
    "            loss = loss_fn(y, t)  # 損失計算\n",
    "            loss.backward()  # 逆伝播\n",
    "            optimizer.step()  # パラメータ更新\n",
    "            prog.update(loss.item())  # 進捗バー更新\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6dd9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67845ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# torch.stackの挙動を確認する．\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = torch.tensor([7, 8, 9])\n",
    "d = torch.stack([a, b, c])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.rnn_cell = RNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        x: (seq_len, batch_size, input_size)\n",
    "        h: (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        hs = []\n",
    "        for xi in x:\n",
    "            h = self.rnn_cell(xi, h)\n",
    "            hs.append(h)\n",
    "        hs = torch.stack(hs)  # (seq_len, batch_size, hidden_size)\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab735dc",
   "metadata": {},
   "source": [
    "内積を解き明かす．\n",
    "\n",
    "query, key, valueを使用した，scaled dot-product attentionについて，その意味を解き明かしたい．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56911b1b",
   "metadata": {},
   "source": [
    "cousal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        # self.scale = d ** (1/2) # 右辺で新たに渡している要素はdのみ\n",
    "        self.scale = 1\n",
    "        self.softmax = nn.Softmax(\n",
    "            dim=-1\n",
    "        )  # batchが入ってきたときに対応できるよう，-1にしておく．softmaxはkey方向に取りたいので，それは横方向．\n",
    "        # ここが，dim=1だとbatch次元が増えたときに，一個ずれてしまう．不本意に縦をとってしまう．\n",
    "\n",
    "    # 左辺をそのまま書く．左辺で新たに渡している要素はQ, K, Vだよね．\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q: seq_length × embedding_size(8×512)\n",
    "            k: seq_length × embedding_size(8×512)\n",
    "            v: seq_length × embedding_size(8×512)\n",
    "        \"\"\"\n",
    "        seq_len = q.shape[-2]\n",
    "\n",
    "        score = (q @ k.mT) / self.scale  # 8×8\n",
    "\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        score = score.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        return self.softmax(score) @ v  # 8×8 @ 8×512 = 8×512"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transforemer-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
