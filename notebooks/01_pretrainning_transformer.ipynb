{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ff4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_fontja\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"globis-university/aozorabunko-clean\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9675187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'footnote', 'meta'],\n",
       "    num_rows: 16951\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fede3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8e422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16951"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f469ff0",
   "metadata": {},
   "source": [
    "物語生成モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd301318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# データセットを直接渡すとメモリを圧迫するため、ジェネレータをかませる\n",
    "def ds_iter():\n",
    "    for item in ds[\"text\"]:\n",
    "        yield item\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    sentence_iterator=ds_iter(),\n",
    "    model_prefix=\"trained/tokenizer/sp_jawiki\",\n",
    "    vocab_size=8000,\n",
    "    model_type=\"unigram\",\n",
    "    character_coverage=0.9995, # どの程度の文字をカバーするか。これより使用頻度の低い文字はUNKになる\n",
    "    train_extremely_large_corpus=True,\n",
    "    unk_id=0,\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    pad_id=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e79636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.11.12)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7fe59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.5.2\n",
      "  latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=25.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (6.33.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--2025-12-07 09:03:04--  https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py\n",
      "raw.githubusercontent.com (raw.githubusercontent.com) をDNSに問いあわせています... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 6257 (6.1K) [text/plain]\n",
      "`sentencepiece_model_pb2.py' に保存中\n",
      "\n",
      "sentencepiece_model 100%[===================>]   6.11K  --.-KB/s 時間 0s         \n",
      "\n",
      "2025-12-07 09:03:04 (48.5 MB/s) - `sentencepiece_model_pb2.py' へ保存完了 [6257/6257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install wget -y\n",
    "!pip install protobuf\n",
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/python/src/sentencepiece/sentencepiece_model_pb2.py\n",
    "\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "tokenizer = SentencePieceUnigramTokenizer.from_spm(\"trained/tokenizer/sp_jawiki.model\")\n",
    "\n",
    "!rm sentencepiece_model_pb2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66616d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=8000, model=SentencePieceUnigram)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909c2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# これをすると、なぜかメタスペースが入ってしまう。\n",
    "# tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=\"<s> $A </s>\", # BOS, EOSで囲む処理を指定\n",
    "#     special_tokens=[\n",
    "#         (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "#         (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     ],\n",
    "# )\n",
    "# tokenizer.save(\"trained/tokenizer/jawiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90bc1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14890f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"trained/tokenizer/jawiki.json\")\n",
    "tokenizer.add_special_tokens({\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f465d1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 45, 1126, 384, 576, 8, 894, 384, 576, 9, 3477, 820, 15, 441]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"人工知能は人間の知能を超えるか？\"\n",
    "ids = tokenizer.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcdc4dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'か?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([7, 45, 1126])\n",
    "tokenizer.decode([15, 441])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa154e38",
   "metadata": {},
   "source": [
    "tokenizerができた。\n",
    "\n",
    "tokenizerはpaddingまで自動でやってくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "361b00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49aaaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d0e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collatorを作る。\n",
    "# dataloaderに渡されたデータはlist型でcollate_fnに渡され、collateの処理が走る。\n",
    "# ここではtokenizerに渡すという処理を噛ませる。\n",
    "def data_collator(batch: list):\n",
    "    return tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    ds[\"text\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2f00f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "797dbec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[2078,   99,  550,  ...,   55, 5775, 1821],\n",
       "        [   7,  114,   88,  ..., 6825,  776,  705],\n",
       "        [   7, 2155,  127,  ..., 2233, 2034,    5],\n",
       "        ...,\n",
       "        [   7, 3198, 6115,  ...,  239,  657, 2311],\n",
       "        [2019,  147,  914,  ...,    3,    3,    3],\n",
       "        [   7, 1326,   47,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239cb9b",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a86f4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26d7396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.scale = d ** (1 / 2)  # 右辺で新たに渡している要素はdのみ\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q: (seq_length, embedding_size)\n",
    "            k: (seq_length, embedding_size)\n",
    "            v: (seq_length, embedding_size)\n",
    "        \"\"\"\n",
    "        seq_len = q.shape[-2]\n",
    "        score = (q @ k.mT) / self.scale\n",
    "        # 増えたところ↓\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        score = score.masked_fill(mask, -torch.inf)  # Trueを-infに飛ばす．\n",
    "\n",
    "        return self.softmax(score) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01e14e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_head, d_model\n",
    "    ):  # ここは64じゃなくてn_headの方が良い．契約プログラミング的に．64とすると全体で見た時マジックナンバーっぽくなる．\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, \"割り切れないよ\"\n",
    "        self.n_head = n_head\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.attention = CausalAttention(d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        qw = self.w_q(q)\n",
    "        kw = self.w_k(k)\n",
    "        vw = self.w_v(v)\n",
    "        calculated = []\n",
    "        # chunkでhead数に分割する．\n",
    "        for qw_i, kw_i, vw_i in zip(\n",
    "            qw.chunk(self.n_head, dim=-1),\n",
    "            kw.chunk(self.n_head, dim=-1),\n",
    "            vw.chunk(self.n_head, dim=-1),\n",
    "        ):\n",
    "            calculated.append(self.attention(qw_i, kw_i, vw_i))\n",
    "        return self.w_o(torch.cat(calculated, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6042700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f50778b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_head, d_model)\n",
    "        self.ffn = FFN(d_model, d_ff)\n",
    "        self.layer_norm_1 = nn.LayerNorm(\n",
    "            d_model\n",
    "        )  # normレイヤーはパラメータを共有しているわけでは無いので，二つインスタンスが必要．\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):  # (batch, seq_len, d_model)\n",
    "        residual_1 = x  # 接続用に保存しておく．\n",
    "        x = self.mha(x, x, x)\n",
    "        x = x + residual_1  # 残差結合\n",
    "        x = self.layer_norm_1(x)  # post norm\n",
    "        residual_2 = x\n",
    "        x = self.ffn(x)\n",
    "        x = x + residual_2\n",
    "        x = self.layer_norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "88b2af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, n_head, d_model, d_ff, n_layers, context_window\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=d_model\n",
    "        )\n",
    "        self.transformer_layers = nn.Sequential(\n",
    "            *[TransformerLayer(n_head, d_model, d_ff) for _ in range(n_layers)]\n",
    "        )  # いけてる？\n",
    "        self.linear = nn.Linear(\n",
    "            d_model, vocab_size\n",
    "        )  # batch_size, seq_len, vocab_sizeになる\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.pe = nn.Parameter(\n",
    "            torch.randn(\n",
    "                [context_window, d_model]\n",
    "            )  # ここvocab_sizeじゃなくてd_modelじゃね？\n",
    "        )  # 上限までベクトルを作って，forwardで削れば良い．\n",
    "\n",
    "    def forward(self, x):  # batch_size, seq_len: int\n",
    "        x = self.embed(x)  # batch_size, seq_len, d_model\n",
    "        x = x + self.pe[:x.size(-2), :] # batch_size, seq_len, d_model\n",
    "        x = self.transformer_layers(x) # batch_size, seq_len, d_model\n",
    "        x = self.linear(x) # batch_size, seq_len, vocab_size\n",
    "        return x # batch_size, seq_len, vocab_size\n",
    "        # return self.softmax(x) # batch_size, seq_len, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88096fe",
   "metadata": {},
   "source": [
    "バッチごとの(文章)、単語ごとの、語彙の 確率分布が出てくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a56aec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "n_head = 8\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "context_window = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83280df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_param=27,638,592\n",
      "model_size=110,554,368\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    vocab_size, n_head, d_model, d_ff, n_layers, context_window\n",
    ")\n",
    "\n",
    "# transformer\n",
    "n_param = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "print(f\"{n_param=:,}\")\n",
    "# 1億2000万のパラメータ数！！！\n",
    "# float32の場合，1パラメータ4バイト換算される．\n",
    "model_size = n_param * 4\n",
    "print(f\"{model_size=:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f654ac3",
   "metadata": {},
   "source": [
    "0.03Bモデルみたいな感じのができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3897801a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): Embedding(8000, 512)\n",
       "  (transformer_layers): Sequential(\n",
       "    (0): TransformerLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention): CausalAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention): CausalAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention): CausalAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention): CausalAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention): CausalAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attention): CausalAttention(\n",
       "          (softmax): Softmax(dim=-1)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=8000, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f04ce3",
   "metadata": {},
   "source": [
    "- input: tokenizeされたデータ\n",
    "- output: sortmaxの確率分布\n",
    "- loss: cross entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a835f010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2078,   99,  550,  ...,   55, 5775, 1821],\n",
       "        [   7,  114,   88,  ..., 6825,  776,  705],\n",
       "        [   7, 2155,  127,  ..., 2233, 2034,    5],\n",
       "        ...,\n",
       "        [   7, 3198, 6115,  ...,  239,  657, 2311],\n",
       "        [2019,  147,  914,  ...,    3,    3,    3],\n",
       "        [   7, 1326,   47,  ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x = sample[\"input_ids\"]\n",
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "98fc517d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2078,   99,  550,  ...,   55, 5775, 1821])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce924cb",
   "metadata": {},
   "source": [
    "$$\n",
    "loss = -\\sum \\log p(x_{t}|x_{<t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c38bf",
   "metadata": {},
   "source": [
    "transformer: n個のsequenceを入れると、n個のsequenceを出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92ac7212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_ids\"][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de1e13d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2078,   99,  550,  ...,   55, 5775, 1821])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d89e70",
   "metadata": {},
   "source": [
    "$$\n",
    "x = x_1, x_2, ..., x_T\n",
    "\\\\\n",
    "y = \\text{transformer}(x) = y_1, y_2, ..., y_T こいつらは確率分布\n",
    "\\\\\n",
    "y_t = \\text{transformer}(x_{<{t}})\n",
    "\\\\\n",
    "y_t = p(x_{{t}}|x_{<{t}})\n",
    "\n",
    "\n",
    "x = x_1, x_2, ..., x_T\n",
    "\n",
    "input:x_1\n",
    "label:x_2\n",
    "\n",
    "input:x_1, x_2\n",
    "label:x_3\n",
    "\n",
    "input:x_1, x_2,...x_t\n",
    "label:x_{t+1}\n",
    "\n",
    "input:x_1, x_2,...x_{T-1}\n",
    "label:x_{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b07bc95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2078,   99,  550,  ...,   55, 5775, 1821])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f73a02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = sample['input_ids'][0].clone()\n",
    "sample_input = sample_input[:-1]\n",
    "\n",
    "sample_super = sample['input_ids'][0].clone()\n",
    "sample_super = sample_super[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3ddb95ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2078,   99])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5e21143f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6817)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_super[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "27308837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = sample_input.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e7866e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2078,   99,  550,  ...,  449,   55, 5775]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb56ed",
   "metadata": {},
   "source": [
    "以下が予測と教師のペア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aba18adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1023, 8000])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(sample_input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d856b671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1023])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_super.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50c78343",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "n_head = 8\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "context_window = 1024\n",
    "model = Transformer(\n",
    "    vocab_size, n_head, d_model, d_ff, n_layers, context_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f7004c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "93c3a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    ds[\"text\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = batch, seq_len, vocab\n",
    "# label = batch, seq_len, vocab\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "81cc806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 91/2119 [01:39<36:50,  1.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(train_dataloader)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data[\"input_ids\"][:-1]\n",
    "        label = data[\"input_ids\"][1:]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.transpose(-2, -1), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f52c",
   "metadata": {},
   "source": [
    "学習させて、推論させてみる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
